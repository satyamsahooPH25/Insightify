# -*- coding: utf-8 -*-
"""Untitled0_(1)_(1)_(2)_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o6e2XqryXcqyDaHgwogr4r3vCwHftrY2
"""

import pandas as pd
import numpy as np
class Blindfold:
    def __init__(self,df,cat_col):
        self.df=df
        self.cat_col=cat_col
        self.Label_Distributor={}
    def splitter(self,cat):
        def Differ(x):
            return "Different"
        change = self.df[self.df[self.df.columns[self.cat_col]]!=cat].sample(frac=1)
        change.iloc[:,self.cat_col] = change.iloc[:,self.cat_col].apply(Differ)
        return self.df[self.df[self.df.columns[self.cat_col]]==cat],change
    def categoriser(self,cat):
        cat_val,Different_val=self.splitter(cat)
        cat_val_len=cat_val.shape[0]
        Different_val_len=Different_val.shape[0]
        Different_stack=[]
        start=0
        end = cat_val_len
        for i in range(Different_val_len//cat_val_len):
            Different_stack.append(Different_val.iloc[start:end])
            start=end
            end=start+cat_val_len
        Different_stack.append(Different_val.iloc[start:Different_val_len])
        for i in range(cat_val_len-(Different_val_len%cat_val_len)):
           Different_stack[-1]=pd.concat([Different_stack[-1],pd.DataFrame(Different_val.iloc[np.random.randint(0,Different_val.shape[0],1)])],ignore_index=True)
        Output=[]
        for i in Different_stack:
            i=pd.concat([i,cat_val],ignore_index=True)
            Output.append(i)
        return Output
    def Dict_Cat(self):
        Dict = {}
        count = 0
        for i in self.df[self.df.columns[self.cat_col]].unique():
            cat_data = self.categoriser(i)
            for k in cat_data:
                Dict[i + str(count)] = k
                count += 1
            count=0
        return Dict

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf

df=pd.read_excel(r"/content/drive/MyDrive/Augmented_Prompt.xlsx")

blindfold_sep=Blindfold(df,0).Dict_Cat()

import gensim
glove_file = r"/content/drive/MyDrive/glove.42B.300d.txt"
print("Loading GloVe model...")
glove_model = gensim.models.KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)



ft = glove_model

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
import nltk
from nltk import sent_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import string
import nltk
from nltk.corpus import stopwords
import gensim
from gensim.utils import simple_preprocess
import nltk
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow import keras
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
import gensim
from gensim.models import Word2Vec
import numpy as np
from sklearn.utils import shuffle
def transform_text(text, use_lemmatization=True):
  text = text.lower()
  tokens = nltk.word_tokenize(text)
  filtered_tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english') and word!='columns' and word!='column'and word!='data' and word!='dataset']
  filtered_tokens = [word for word in filtered_tokens if word not in string.punctuation]
  if use_lemmatization:
      wnl = nltk.WordNetLemmatizer()
      lemmatized_tokens = [wnl.lemmatize(word) for word in filtered_tokens]
  else:
      ps = PorterStemmer()
      lemmatized_tokens = [ps.stem(word) for word in filtered_tokens]

  text = " ".join(lemmatized_tokens)
  return text
df["Question"] = df["Question"].apply(transform_text)
sentences=[]
max_len=0
for j in df["Question"]:
  sentences.append(j.split(" "))
  if(len(j.split(" "))>max_len):
    max_len=len(j.split(" "))
model = ft
counter=0
for i in blindfold_sep:
  counter+=1
  df=blindfold_sep[i]
  y = df['Type']
  le = LabelEncoder()
  y_encoded = le.fit_transform(y)
  y = to_categorical(y_encoded, num_classes=2)
  df["Question"] = df["Question"].apply(transform_text)
  sentences=[]
  for j in df["Question"]:
    sentences.append(j.split(" "))
  Embedded_sentences=[]
  for tokens in sentences:
      embsum=np.zeros(300)
      embeddings=[]
      for count,token in enumerate(tokens):
        try:
          embeddings.append(model[token])
          embsum+=model[token]
        except:
          embeddings.append(embsum/(count+1))
      while(len(embeddings)<max_len):
        padding_array = np.zeros(300,)
        embeddings.append(padding_array)
      Embedded_sentences.append(np.array(embeddings))
  Embedded_sentences=np.array(Embedded_sentences)
  Embedded_sentences, y_train = shuffle(Embedded_sentences, y, random_state=42)
  early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)
  model_checkpoint_callback = ModelCheckpoint(
        filepath=f"/content/drive/MyDrive/best_till_day/{i}_best",
        monitor='val_accuracy',
        mode='max',  # Save only if validation accuracy improves
        save_best_only=True  # Save only the best model based on the monitored metric
    )
  model1 = Sequential()
  model1.add(LSTM(256, return_sequences=True, input_shape=(max_len,300)))
  model1.add(Dropout(0.4))
  model1.add(LSTM(32))
  model1.add(Dropout(0.4))
  model1.add(Dense(5, activation='relu'))
  model1.add(Dense(2, activation='softmax'))
  model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  history = model1.fit(Embedded_sentences, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping,model_checkpoint_callback])
  # model1.save(f"/content/drive/MyDrive/{i}")
  with open(f"/content/drive/MyDrive/best_till_day/{i}_best_class_labels.txt", 'w') as f:
        for label in le.classes_:
            f.write(f"{label}\n")

glove_model.save('/content/drive/MyDrive/best_till_day/glove_model.gensim')



